We go after websites because 
    most organizations have them
    they are public facing
    in some circumstances (esp self hosted) they provide ingress
    large attack surface

Understand basic web communication
Familiarization wtih Developer Console
**Conduct Cross-site scripting
**Conduct URL command injection

HTTP - how the data is getting to your browser - rides on TCP
    Different HTTP versions don't change the attacks
HTML - the content - one layer deeper (presentation)

Will not have https targets - doesn't functionally change the attacks we will be using, just blocks interlopers

tcpdump and wireshare are viable for headers, but excessive
Will be using the developer console (right-click and inspect)
    Networking tab

HTTP Methods
    GET 
        Pull data from the server
        4000 character limit
    POST 
        Send data to the server 
        many megabytes of data limit
    HEAD 
        Just the header data        
    PUT 

In the header
User-Agent: can have a lot of data about a potential target - browser, OS, processor architecture 
Referer: how you got there - ex. google.com, self referred. Can use the referer section to deny/allow access (ie bank website, you didn't come from the login page...)
Cookie: identify a specific user or session -- companies have made this much more powerful
Server: Apache, NGINX
Set-Cookie: store a new cookie

GET request - changes the URL
*.php? <-- parameter
Can manipulate the GET request in the URL bar

wget - follows the href links